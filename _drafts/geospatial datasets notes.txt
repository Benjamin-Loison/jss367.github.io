
Having robust benchmarking datasets is a critical 

Benchmarking datasets are hugely important to the development of neural networks, and ImageNet in particular is significant in the improvements in computer vision over the last five years. Given the importance of benchmarking datasets, is it any wonder that domains without that don't seem to cultivate innovation at the same levels? In particular, geospatial analytics doesn't have nearly the level of new networks that one would imagine given the importance, and hopefully the DOTA dataset can change that.

First, why does geospatial analytics even need it's own dataset? Can't you just fine-tune off of ImageNet weights? Well, yes, you can, and you can even get pretty far that way. But there are important differences between overhead images and the "normal" images we see in ImageNet or a similar benchmarking dataset.

Here are some of the key differences between overhead and natural imagery:

* Arbitrary orientations
** These are significant factors because they influence the architectures that will be developed. Many architectures (YOLO, Faster-RCNN) have a bias towards horizontally oriented objects. This is not acceptable when in overhead imagery.
* Greater scale variation
** The scale matters because anchor sized are often-prechoosen and can't be easily changed.
* Objects are more densely packed cluttered
** large aspect ratios - bridges, ships, roads, etc.
** The denser packed objects make NMS more difficult
* More class imbalance
** possibly because real life is more unbalanced too but the public datasets correct for this. but it's harder to correct in overhead so they don't

way more instances per image
PASCAL VOC - 2.89 instances per image
MSCOCO - 7.19 (they have slightly different numbers elsewhere, good to check?)
ImageNet - 1.37
DOTA - 67.10


Large datasets are especially important for geospatial analytics, because most algorithms are pre-trained from natural images, such as ImageNet and MS COCO.


There is a need for overhead imagery datasets. Natural imagery doesn't have the same distributions.


what to judge a dataset on:
 1) a large number of images, 2) many instances per categories, 3) properly oriented object annotation, and
4) many different classes of objects, which make it approach to real-world applications.




Dataset 					Annotation way 	#main categories 	#Instances 	#Images 	Image width
NWPU VHR-10 [2] 			horizontal BB 	10 					3651 		800			∼1000
SZTAKI-INRIA [1] 			oriented BB 	1 					665 		9 			∼800
TAS [9] 					horizontal BB 	1					1319		30			792
COWC [20] 					one dot 		1 					32716 		53 			2000∼19,000
VEDAI [24] 					oriented BB 	3 					2950 		1268 		512, 1024
UCAS-AOD [39] 				oriented BB 	2 					14,596 		1510 		∼1000
HRSC2016 [17] 				oriented BB 	1 					2976 		1061 		∼1100
3K Vehicle Detection [15] 	oriented BB 	2 					14,235 		20 			5616
DOTA 						oriented BB 	14					188,282 	2806 		800∼4000

More notes:




DOTA
introduce a large-scale Dataset for Object deTection in Aerial images (DOTA)
- 2806 aerial images from
different sensors and platforms. Each image is of the size about 4000 × 4000 pixels and contains
objects exhibiting a wide variety of scales, orientations, and shapes.
-  15 common object categories



Most datasets are from Google Earth imagery, so consist of RGB.



Datasets like TAS [9], VEDAI [24], COWC [20] and DLR 3K Munich Vehicle [15] only focus on vehicles. UCAS-AOD [39] contains vehicles and planes while HRSC2016 [17] only contains ships even though fine-grained category information are given. 



